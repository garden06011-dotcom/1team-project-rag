# #!/usr/bin/env python
# # -*- coding: utf-8 -*-
# """
# ë¬¸ì„œ ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸

# data/documents/ í´ë”ì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ì½ì–´ì„œ
# ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
# """

# import os
# import sys
# from pathlib import Path

# # í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
# sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# from rag.document_loader import DirectoryLoader, TextSplitter
# from rag.embeddings import BGEEmbeddings
# from rag.vector_store import ChromaVectorStore


# def main():
#     print("=" * 70)
#     print("ğŸ“š ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘")
#     print("=" * 70)

#     # 1. ë¬¸ì„œ í´ë” ê²½ë¡œ
#     documents_path = Path(__file__).parent / "data" / "documents"
#     print(f"\nğŸ“ ë¬¸ì„œ í´ë”: {documents_path}")

#     # 2. ë¬¸ì„œ ë¡œë“œ
#     print("\nğŸ” 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë”© ì¤‘...")
#     loader = DirectoryLoader(
#         directory_path=str(documents_path),
#         supported_extensions=[".txt", ".pdf", ".docx", ".md"]
#     )

#     try:
#         documents = loader.load()
#         print(f"   âœ“ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")

#         if not documents:
#             print("\nâš ï¸  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. data/documents/ í´ë”ì— ë¬¸ì„œë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
#             return

#         # ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°
#         print("\nğŸ“„ ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡:")
#         for i, doc in enumerate(documents, 1):
#             source = doc.metadata.get("source", "unknown")
#             content_preview = doc.page_content[:50].replace("\n", " ")
#             print(f"   {i}. {source} - {content_preview}...")

#     except Exception as e:
#         print(f"\nâŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
#         return

#     # 3. í…ìŠ¤íŠ¸ ë¶„í• 
#     print("\nâœ‚ï¸  2ë‹¨ê³„: í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
#     splitter = TextSplitter(
#         chunk_size=300,      # 500ì ë‹¨ìœ„ë¡œ ë¶„í• 
#         chunk_overlap=100,   # 100ì ì˜¤ë²„ë©
#         separator="\n\n"     # ë¬¸ë‹¨ ê¸°ì¤€ ë¶„í• 
#     )

#     try:
#         split_docs = splitter.split_documents(documents)
#         print(f"   âœ“ {len(split_docs)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

#         # ì²­í¬ í†µê³„
#         total_chars = sum(len(doc.page_content) for doc in split_docs)
#         avg_chars = total_chars / len(split_docs) if split_docs else 0
#         print(f"   - ì´ ë¬¸ì ìˆ˜: {total_chars:,}ì")
#         print(f"   - í‰ê·  ì²­í¬ í¬ê¸°: {avg_chars:.0f}ì")

#     except Exception as e:
#         print(f"\nâŒ í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤íŒ¨: {e}")
#         return

#     # 4. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
#     print("\nğŸ¤– 3ë‹¨ê³„: BGE-M3-KO ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
#     try:
#         embeddings_model = BGEEmbeddings()
#         print(f"   âœ“ ì„ë² ë”© ì°¨ì›: {embeddings_model.get_embedding_dimension()}")
#     except Exception as e:
#         print(f"\nâŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
#         return

#     # 5. ë¬¸ì„œ ì„ë² ë”©
#     print("\nğŸ”¢ 4ë‹¨ê³„: ë¬¸ì„œ ì„ë² ë”© ì¤‘...")
#     try:
#         texts = [doc.page_content for doc in split_docs]
#         metadatas = [doc.metadata for doc in split_docs]

#         print(f"   - {len(texts)}ê°œ ì²­í¬ ì„ë² ë”© ì‹œì‘...")
#         print("   (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...)")

#         doc_embeddings = embeddings_model.embed_documents(texts)
#         print(f"   âœ“ {len(doc_embeddings)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")

#     except Exception as e:
#         print(f"\nâŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
#         return

#     # 6. ChromaDBì— ì €ì¥
#     print("\nğŸ’¾ 5ë‹¨ê³„: ChromaDBì— ì €ì¥ ì¤‘...")
#     try:
#         # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
#         vector_store = ChromaVectorStore(
#             collection_name="commercial_analysis_docs"
#         )

#         # [ë³€ê²½] ê¸°ì¡´ ë°ì´í„° ê°œìˆ˜ë§Œ ì¡°íšŒí•˜ê³ , ì‚­ì œëŠ” í•˜ì§€ ì•ŠìŒ
#         # existing_count = vector_store.get_document_count()
#         # if existing_count > 0:
#         #     print(f"   ğŸ” ê¸°ì¡´ ë°ì´í„° {existing_count}ê°œ ì¡´ì¬ (ìœ ì§€ë¨)")  # [ë³€ê²½ëœ ë¡œê·¸]
#         #     print("   ğŸ” append ëª¨ë“œë¡œ ìƒˆ ë¬¸ì„œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")       # [ì¶”ê°€]

#         # # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚­ì œ
#         existing_count = vector_store.get_document_count()
#         if existing_count > 0:
#             print(f"   âš ï¸  ê¸°ì¡´ ë°ì´í„° {existing_count}ê°œ ë°œê²¬")
#             print("   - ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì¤‘...")
#             vector_store.delete_collection()

#             # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
#             vector_store = ChromaVectorStore(
#                 collection_name="commercial_analysis_docs"
#             )
#             print("   âœ“ ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")

#         # ë¬¸ì„œ ì¶”ê°€
#         ids = vector_store.add_documents(
#             texts=texts,
#             embeddings=doc_embeddings,
#             metadatas=metadatas
#         )

#         print(f"   âœ“ ìƒˆë¡œ ì¶”ê°€ëœ ë¬¸ì„œ ìˆ˜: {len(ids)}ê°œ")  # [ë³€ê²½: ë¬¸êµ¬ ëª…í™•í™”]

#         # print(f"   âœ“ {len(ids)}ê°œ ë¬¸ì„œ ChromaDBì— ì €ì¥ ì™„ë£Œ")

#     except Exception as e:
#         print(f"\nâŒ ChromaDB ì €ì¥ ì‹¤íŒ¨: {e}")
#         return

#     # 7. ê²€ì¦
#     print("\nâœ… 6ë‹¨ê³„: ì¸ë±ì‹± ê²€ì¦ ì¤‘...")
#     try:
#         final_count = vector_store.get_document_count()
#         print(f"   âœ“ ìµœì¢… ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {final_count}ê°œ")

#         # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
#         print("\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ìˆ˜í–‰...")
#         test_query = "ê°•ë‚¨ì—ì„œ ì¹´í˜ ì°½ì—…"
#         test_embedding = embeddings_model.embed_query(test_query)
#         results = vector_store.search(test_embedding, top_k=3)

#         print(f"   - ê²€ìƒ‰ ì¿¼ë¦¬: '{test_query}'")
#         print(f"   - ê²€ìƒ‰ ê²°ê³¼: {len(results['documents'])}ê°œ")

#         if results["documents"]:
#             print("\n   ğŸ“„ ê²€ìƒ‰ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
#             for i, (doc, metadata, distance) in enumerate(zip(
#                 results["documents"][:2],
#                 results["metadatas"][:2],
#                 results["distances"][:2]
#             ), 1):
#                 similarity = 1 - (distance / 2)
#                 source = metadata.get("source", "unknown")
#                 preview = doc[:80].replace("\n", " ")
#                 print(f"   [{i}] {source} (ìœ ì‚¬ë„: {similarity:.3f})")
#                 print(f"       {preview}...")

#     except Exception as e:
#         print(f"\nâš ï¸  ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")

#     # ì™„ë£Œ
#     print("\n" + "=" * 70)
#     print("ğŸ‰ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ!")
#     print("=" * 70)
#     print(f"\nâœ… ì´ {len(split_docs)}ê°œ ë¬¸ì„œ ì²­í¬ê°€ ë²¡í„° DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
#     print(f"âœ… ì´ì œ RAG ì±—ë´‡ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
#     print(f"\nğŸ’¡ ì„œë²„ ì‹œì‘: cd backend && uvicorn main:app --reload --port 8001")
#     print(f"ğŸ’¡ í…ŒìŠ¤íŠ¸: POST http://localhost:8001/api/rag-chat-stream")
#     print()


# if __name__ == "__main__":
#     try:
#         main()
#     except KeyboardInterrupt:
#         print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
#         sys.exit(0)
#     except Exception as e:
#         print(f"\n\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         import traceback
#         traceback.print_exc()
#         sys.exit(1)

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ë¬¸ì„œ ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸ (RAG)
- data/documents í´ë”ì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì™€ í…ìŠ¤íŠ¸ ë¶„í• 
- ì„ë² ë”© ìƒì„± í›„ ChromaDBì— ì €ì¥
"""

import os
import sys
from pathlib import Path

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from rag.document_loader import DirectoryLoader, TextSplitter
from rag.embeddings import BGEEmbeddings
from rag.vector_store import ChromaVectorStore


# -------------------------------------------------------------------
# ğŸ”¥ ChromaDBëŠ” batch í¬ê¸°ê°€ ë„ˆë¬´ í¬ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•œë‹¤
#    â†’ batch size ìë™ ë¶„í•  ê¸°ëŠ¥ ì¶”ê°€
# -------------------------------------------------------------------
def split_into_batches(items, batch_size):
    """ë¦¬ìŠ¤íŠ¸ë¥¼ batch ë‹¨ìœ„ë¡œ ë¶„í• """
    for i in range(0, len(items), batch_size):
        yield items[i:i + batch_size]


def main():
    print("=" * 70)
    print("ğŸ“š ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘")
    print("=" * 70)

    # -------------------------------------------------------------------
    # 1. ë¬¸ì„œ í´ë”
    # -------------------------------------------------------------------
    documents_path = Path(__file__).parent / "data" / "documents"
    print(f"\nğŸ“ ë¬¸ì„œ í´ë”: {documents_path}")

    # -------------------------------------------------------------------
    # 2. ë¬¸ì„œ ë¡œë“œ
    # -------------------------------------------------------------------
    print("\nğŸ” 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë”© ì¤‘...")
    loader = DirectoryLoader(
        directory_path=str(documents_path),
        supported_extensions=[".txt", ".pdf", ".docx", ".md"]
    )

    try:
        documents = loader.load()
        print(f"   âœ“ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")

        if not documents:
            print("âš ï¸  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. data/documents/ í´ë”ì— ë¬¸ì„œë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
            return

        print("\nğŸ“„ ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡:")
        for i, doc in enumerate(documents, 1):
            preview = doc.page_content[:50].replace("\n", " ")
            print(f"   {i}. {doc.metadata.get('source', '?')} - {preview}...")

    except Exception as e:
        print(f"\nâŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # -------------------------------------------------------------------
    # 3. í…ìŠ¤íŠ¸ ë¶„í• 
    # -------------------------------------------------------------------
    print("\nâœ‚ï¸  2ë‹¨ê³„: í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
    splitter = TextSplitter(
        chunk_size=300,
        chunk_overlap=100,
        separator="\n\n"
    )

    try:
        split_docs = splitter.split_documents(documents)
        print(f"   âœ“ {len(split_docs)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

        total_chars = sum(len(doc.page_content) for doc in split_docs)
        avg_chars = total_chars / len(split_docs)
        print(f"   - ì´ ë¬¸ì ìˆ˜: {total_chars:,}ì")
        print(f"   - í‰ê·  ì²­í¬ í¬ê¸°: {avg_chars:.0f}ì")

    except Exception as e:
        print(f"\nâŒ í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤íŒ¨: {e}")
        return

    # -------------------------------------------------------------------
    # 4. ì„ë² ë”© ëª¨ë¸ ë¡œë”©
    # -------------------------------------------------------------------
    print("\nğŸ¤– 3ë‹¨ê³„: BGE-M3-KO ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
    try:
        embeddings_model = BGEEmbeddings()
        print(f"   âœ“ ì„ë² ë”© ì°¨ì›: {embeddings_model.get_embedding_dimension()}")

    except Exception as e:
        print(f"\nâŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # -------------------------------------------------------------------
    # 5. ë¬¸ì„œ ì„ë² ë”© ìƒì„±
    # -------------------------------------------------------------------
    print("\nğŸ”¢ 4ë‹¨ê³„: ë¬¸ì„œ ì„ë² ë”© ì¤‘...")
    texts = [d.page_content for d in split_docs]
    metadatas = [d.metadata for d in split_docs]

    try:
        print(f"   - ì´ {len(texts)}ê°œ ì²­í¬ ì„ë² ë”© ì‹œì‘...")
        doc_embeddings = embeddings_model.embed_documents(texts)
        print(f"   âœ“ ì„ë² ë”© ìƒì„± ì™„ë£Œ")

    except Exception as e:
        print(f"\nâŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return

    # -------------------------------------------------------------------
    # 6. ChromaDB ì €ì¥
    # -------------------------------------------------------------------
    print("\nğŸ’¾ 5ë‹¨ê³„: ChromaDBì— ì €ì¥ ì¤‘...")

    try:
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ìƒˆë¡œ ì‹œì‘
        vector_store = ChromaVectorStore(collection_name="commercial_analysis_docs")

        existing = vector_store.get_document_count()
        if existing > 0:
            print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° {existing}ê°œ ë°œê²¬ â†’ ì‚­ì œí•©ë‹ˆë‹¤.")
            vector_store.delete_collection()
            vector_store = ChromaVectorStore(collection_name="commercial_analysis_docs")
            print("âœ“ ìƒˆ ì»¬ë ‰ì…˜ ì¬ìƒì„± ì™„ë£Œ")

        # ğŸ”¥ batch ì €ì¥ ì²˜ë¦¬
        BATCH = 1000  # ì•ˆì •ì ì¸ ë°°ì¹˜ í¬ê¸°
        print(f"\nğŸ“¦ batch ì €ì¥ ì‹œì‘ (batch í¬ê¸°: {BATCH})")

        total_added = 0
        for batch_texts, batch_embs, batch_metas in zip(
            split_into_batches(texts, BATCH),
            split_into_batches(doc_embeddings, BATCH),
            split_into_batches(metadatas, BATCH)
        ):
            ids = vector_store.add_documents(
                texts=batch_texts,
                embeddings=batch_embs,
                metadatas=batch_metas
            )
            total_added += len(ids)
            print(f"   - Batch ì €ì¥ ì™„ë£Œ (ëˆ„ì : {total_added})")

        print(f"\n   âœ“ ìµœì¢… ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {total_added}ê°œ")

    except Exception as e:
        print(f"\nâŒ ChromaDB ì €ì¥ ì‹¤íŒ¨: {e}")
        return

    # -------------------------------------------------------------------
    # 7. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    # -------------------------------------------------------------------
    print("\nğŸ” 6ë‹¨ê³„: RAG ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì¤‘...")

    try:
        query = "ê°•ë‚¨ì—ì„œ ì°½ì—…"
        query_emb = embeddings_model.embed_query(query)
        res = vector_store.search(query_emb, top_k=3)

        print(f"   - ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
        print(f"   - ê²°ê³¼ ê°œìˆ˜: {len(res['documents'])}")

        for i in range(len(res["documents"])):
            print(f"\n   [{i+1}] {res['metadatas'][i].get('source')}")
            print(f"        {res['documents'][i][:100]}...")

    except Exception as e:
        print(f"âš ï¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")

    # -------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("ğŸ‰ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ!")
    print("=" * 70)
    print(f"\nğŸ’¡ ì„œë²„ ì‹¤í–‰:  uvicorn main:app --reload --port 8001\n")


# ----------------------------------------------------------
# ì‹¤í–‰ ì‹œì‘
# ----------------------------------------------------------
if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâ›” ì‹¤í–‰ ì¤‘ë‹¨ (ì‚¬ìš©ì)")
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
