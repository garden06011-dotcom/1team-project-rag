# FastAPI ì„œë²„ ë©”ì¸ íŒŒì¼
# RAG ê¸°ë°˜ ì±—ë´‡ ì„œë²„

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from dotenv import load_dotenv
from typing import Optional, List, Dict, Any
import os
import json

# RAG ëª¨ë“ˆ import
from rag.rag_chain import RAGChain
from rag.retriever import Retriever

# ============================================
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# ============================================
load_dotenv()

# ============================================
# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (Lazy Loading)
# ============================================
rag_chain = None

def get_rag_chain():
    """RAG ì²´ì¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (Lazy Loading)"""
    global rag_chain
    if rag_chain is None:
        print("ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘... (ì²« ìš”ì²­, 10~20ì´ˆ ì†Œìš”)")
        
        # Retriever ì´ˆê¸°í™” (í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©)
        retriever = Retriever(
            top_k=int(os.getenv("RAG_TOP_K", "3")),
            score_threshold=float(os.getenv("RAG_SCORE_THRESHOLD", "0.65"))
        )
        
        # RAG Chain ì´ˆê¸°í™”
        rag_chain = RAGChain(
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            retriever=retriever,
            model_name=os.getenv("RAG_MODEL_NAME", "gpt-4o-mini"),
            temperature=float(os.getenv("RAG_TEMPERATURE", "0.7")),
            max_tokens=int(os.getenv("RAG_MAX_TOKENS", "1000"))
        )
        print("âœ… RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    return rag_chain

# ============================================
# FastAPI ì•± ìƒì„±
# ============================================
app = FastAPI(
    title="1team RAG Chatbot API",
    description="RAG ê¸°ë°˜ ìƒê¶Œ ë¶„ì„ ì±—ë´‡ ë°±ì—”ë“œ",
    version="1.0.0"
)

# ============================================
# CORS ì„¤ì •
# ============================================
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",  # Next.js í”„ë¡ íŠ¸ì—”ë“œ
        "http://localhost:8000",  # Node.js ë°±ì—”ë“œ
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# Pydantic ëª¨ë¸ (ë°ì´í„° ê²€ì¦)
# ============================================
class ChatRequest(BaseModel):
    """
    í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë³´ë‚´ëŠ” ìš”ì²­ í˜•ì‹
    """
    message: str  # ì‚¬ìš©ì ë©”ì‹œì§€
    conversation_history: Optional[List[Dict[str, str]]] = None  # ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒì )

# ============================================
# ì—”ë“œí¬ì¸íŠ¸: ì„œë²„ ìƒíƒœ ì²´í¬
# ============================================
@app.get("/")
async def root():
    return {
        "message": "1team RAG Chatbot API",
        "status": "healthy",
        "version": "1.0.0"
    }

@app.get("/health")
async def health_check():
    return {"status": "ok"}

# ============================================
# RAG ì±—ë´‡ ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸
# ============================================

async def stream_rag_response(
    query: str,
    conversation_history: Optional[List[Dict[str, str]]] = None,
    top_k: int = 3
):
    """
    RAG ì‘ë‹µì„ SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ì†¡
    """
    try:
        # RAG ì²´ì¸ ê°€ì ¸ì˜¤ê¸°
        rag = get_rag_chain()

        # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        for chunk in rag.stream_run(
            query=query,
            conversation_history=conversation_history,
            top_k=top_k
        ):
            # SSE í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì „ì†¡
            chunk_type = chunk.get("type")
            content = chunk.get("content")

            if chunk_type == "sources":
                # ì°¸ê³  ë¬¸ì„œ ì •ë³´ ì „ì†¡
                yield f"data: {json.dumps({'event': 'sources', 'sources': content}, ensure_ascii=False)}\n\n"
            elif chunk_type == "answer":
                # ë‹µë³€ ì²­í¬ ì „ì†¡
                yield f"data: {json.dumps({'event': 'answer', 'content': content}, ensure_ascii=False)}\n\n"
            elif chunk_type == "error":
                # ì—ëŸ¬ ì „ì†¡
                yield f"data: {json.dumps({'event': 'error', 'message': content}, ensure_ascii=False)}\n\n"

        # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
        yield f"data: {json.dumps({'event': 'done'}, ensure_ascii=False)}\n\n"

    except Exception as e:
        error_msg = json.dumps({
            "event": "error",
            "message": f"RAG ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}"
        }, ensure_ascii=False)
        yield f"data: {error_msg}\n\n"


@app.post("/api/rag-chat-stream")
async def rag_chat_stream(request: ChatRequest):
    """
    RAG ì±—ë´‡ ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ (SSE)

    ì‘ë™ ë°©ì‹:
    1. ì‚¬ìš©ì ì§ˆë¬¸ ë°›ê¸°
    2. ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    3. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ OpenAI API ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
    4. ì‹¤ì‹œê°„ ë‹µë³€ + ì°¸ê³  ë¬¸ì„œ ë°˜í™˜
    """
    print("\n" + "="*50)
    print("ğŸ“¥ [RAG Stream] ë°›ì€ ìš”ì²­:")
    print(f"  - query: {request.message[:50]}...")
    print(f"  - history: {len(request.conversation_history) if request.conversation_history else 0}ê°œ")
    print("="*50 + "\n")

    return StreamingResponse(
        stream_rag_response(
            query=request.message,
            conversation_history=request.conversation_history,
            top_k=int(os.getenv("RAG_TOP_K", "3"))
        ),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

