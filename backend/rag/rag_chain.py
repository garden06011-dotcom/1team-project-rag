"""
RAG (Retrieval-Augmented Generation) íŒŒì´í”„ë¼ì¸

ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

from typing import List, Dict, Any, Optional
from openai import OpenAI
import os
from .retriever import Retriever
from .embeddings import BGEEmbeddings
from .vector_store import ChromaVectorStore


class RAGChain:
    """RAG íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""

    def __init__(
        self,
        openai_api_key: str = None,
        retriever: Retriever = None,
        model_name: str = "gpt-4o-mini",
        temperature: float = 0.7,
        max_tokens: int = 1000
    ):
        """
        RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

        Args:
            openai_api_key: OpenAI API í‚¤
            retriever: ê²€ìƒ‰ê¸° ì¸ìŠ¤í„´ìŠ¤
            model_name: OpenAI ëª¨ë¸ ì´ë¦„
            temperature: ìƒì„± ì˜¨ë„ (0~2)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
        """
        # OpenAI API í‚¤ ì„¤ì •
        if openai_api_key is None:
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if not openai_api_key:
                raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        # proxies ì¸ì ë¬¸ì œ í•´ê²°: httpx í´ë¼ì´ì–¸íŠ¸ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
        try:
            import httpx
            # httpx í´ë¼ì´ì–¸íŠ¸ ìƒì„± (proxies ì—†ì´)
            http_client = httpx.Client(
                timeout=httpx.Timeout(60.0, connect=10.0),
                follow_redirects=True,
            )
            self.client = OpenAI(
                api_key=openai_api_key,
                http_client=http_client
            )
        except ImportError:
            # httpxê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì´ˆê¸°í™”
            self.client = OpenAI(api_key=openai_api_key)
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens

        # ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        if retriever is None:
            print("ğŸ”§ ê¸°ë³¸ Retriever ì´ˆê¸°í™” ì¤‘...")
            self.retriever = Retriever()
        else:
            self.retriever = retriever

        print(f"[OK] RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ (ëª¨ë¸: {model_name})")

    def create_prompt(
        self,
        query: str,
        retrieved_docs: List[Dict[str, Any]],
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> List[Dict[str, str]]:
        """
        RAG í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
            conversation_history: ëŒ€í™” ê¸°ë¡

        Returns:
            OpenAI ë©”ì‹œì§€ í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸
        """
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = """ë‹¹ì‹ ì€ ìƒê¶Œ ë¶„ì„ ë° ì°½ì—… ì»¨ì„¤íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ì£¼ì˜ì‚¬í•­:
1. ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ë˜, ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
2. ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ìë£Œì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ë§í•´ì£¼ì„¸ìš”.
3. ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.
4. í•„ìš”ì‹œ ì¶œì²˜ë¥¼ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
"""

        # ê²€ìƒ‰ëœ ë¬¸ì„œ í¬ë§·íŒ…
        context = self.retriever.format_documents_for_prompt(retrieved_docs)

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        user_prompt = f"""[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ìœ„ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]

        # ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (ìˆìœ¼ë©´)
        if conversation_history:
            messages.extend(conversation_history)

        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": user_prompt})

        return messages

    def run(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        top_k: int = 3
    ) -> Dict[str, Any]:
        """
        RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ëŒ€í™” ê¸°ë¡
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜

        Returns:
            {
                "answer": "LLM ë‹µë³€",
                "sources": [{...}, {...}],  # ì°¸ê³  ë¬¸ì„œ
                "query": "ì›ë³¸ ì§ˆë¬¸"
            }
        """
        print(f"\n[SEARCH] RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘: {query}")

        # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        print(f"[DOCS] 1ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰ (Top-{top_k})...")
        retrieved_docs = self.retriever.search(query, top_k=top_k)

        if not retrieved_docs:
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?",
                "sources": [],
                "query": query
            }

        print(f"   âœ“ {len(retrieved_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")

        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        print(f"[STEP] 2ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±...")
        messages = self.create_prompt(query, retrieved_docs, conversation_history)

        # 3. LLM í˜¸ì¶œ
        print(f"[AI] 3ë‹¨ê³„: LLM ë‹µë³€ ìƒì„±...")
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )

            answer = response.choices[0].message.content
            print(f"   âœ“ ë‹µë³€ ìƒì„± ì™„ë£Œ (í† í°: {response.usage.total_tokens})")

            return {
                "answer": answer,
                "sources": retrieved_docs,
                "query": query,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }

        except Exception as e:
            print(f"[ERROR] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": retrieved_docs,
                "query": query
            }

    def stream_run(
        self,
        query: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        top_k: int = 3
    ):
        """
        RAG íŒŒì´í”„ë¼ì¸ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ëŒ€í™” ê¸°ë¡
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜

        Yields:
            ë‹µë³€ ì²­í¬ ë˜ëŠ” ë©”íƒ€ë°ì´í„°
        """
        print(f"\n[SEARCH] RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ìŠ¤íŠ¸ë¦¬ë°): {query}")

        # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs = self.retriever.search(query, top_k=top_k)

        if not retrieved_docs:
            yield {
                "type": "answer",
                "content": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
            return

        # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ ë¨¼ì € ë°˜í™˜
        yield {
            "type": "sources",
            "content": retrieved_docs
        }

        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        messages = self.create_prompt(query, retrieved_docs, conversation_history)

        # 3. LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        try:
            stream = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                stream=True
            )

            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield {
                        "type": "answer",
                        "content": chunk.choices[0].delta.content
                    }

        except Exception as e:
            print(f"[ERROR] LLM ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
            yield {
                "type": "error",
                "content": str(e)
            }


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # RAG ì²´ì¸ ì´ˆê¸°í™”
    rag_chain = RAGChain()

    # ì§ˆë¬¸
    query = "ê°•ë‚¨ì—ì„œ ì¹´í˜ë¥¼ ì°½ì—…í•˜ë ¤ê³  í•˜ëŠ”ë° ì–´ë–¤ ì ì„ ê³ ë ¤í•´ì•¼ í•˜ë‚˜ìš”?"

    # ì‹¤í–‰
    result = rag_chain.run(query, top_k=3)

    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ì§ˆë¬¸: {result['query']}")
    print(f"\në‹µë³€:\n{result['answer']}")
    print(f"\nì°¸ê³  ë¬¸ì„œ ({len(result['sources'])}ê°œ):")
    for i, source in enumerate(result['sources']):
        print(f"  [{i+1}] {source['metadata'].get('source', 'unknown')} (ìœ ì‚¬ë„: {source['score']:.3f})")

